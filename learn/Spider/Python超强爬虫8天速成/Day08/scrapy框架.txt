scrapy框架

- 什么是框架？
  - 集成了很多功能并且有很强通用性的一个项目模版

- 如何学习框架？
  -  专门学习框架封装的各种功能的详细用法
- 什么是scrapy
  - 爬虫这种封装好的一个明星框架。功能：高性能持久化存储，异步数据下载，高性能数据解析，分布式

- scrapy框架基本使用
  - 安装scrapy
    - pip install scrapy
    - 测试是否安装成功：scrapy

  - 创建一个工程：scrapy startproject 工程名
  - cd xx/xx/工程名
  - 在scrapy子目录中创建一个爬虫文件
    - scrapy genspider 爬虫名 域名
  - 运行爬虫：scrapy crawl 爬虫名

- scrapy数据解析

- scrapy持久化存储
  - 基于终端指令
    - 要求：只可以将parse方法的返回值存储到本地文本文件中
    - 注意：持久化存储对应的文本文件类型只能是：json、csv、xml、pickle等
    - 指令：scrapy crawl 爬虫名 -o 文件名.json
    - 好处：简洁高效
    - 坏处：局限性较强，只能存储到本地，不能存储到数据库中
  - 基于管道
    - 编码流程：
      - 数据解析
      - 在item类中定义相关属性
      - 将解析的数据封装存储到item类型的对象中
      - 将item对象传递给管道进行持久化存储的操作
      - 在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作
      - 在配置文件中开启管道
    - 好处：灵活性强，可以存储到数据库中
    - 坏处：编码流程较复杂，需要编写代码
  - 面试题：将爬取到的数据一份存储到本地，一份存储到数据库该如何实现？
    - 管道文件中一个管道类对应的事将数据存储到一种平台
    - 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接受
    - process_item方法中的return item表示将item传递给下一个即将被执行的管道类

- 基于Spider的全站数据爬取
  - 将网站中某板块下的全部页码对应的页面数据进行爬取
  - 实现思路：
    - 首先，创建爬虫文件，继承scrapy.Spider类
    - 然后，在settings.py文件中设置该爬虫的起始URL和爬取深度
    - 实现parse方法，在该方法中解析页面数据，并返回scrapy.Item对象
    - 实现start_requests方法，在该方法中生成初始请求，并返回Request对象

- 五大核心组件
  - 引擎
      - 负责控制数据流动，调度各个组件，指挥各个组件执行爬虫任务
  - 下载器
      - 负责获取网页数据，并将其传递给解析器
  - Spider
      - 负责解析网页数据，提取数据，并生成请求对象
  - 解析器
      - 负责解析网页数据，提取数据，并生成Item对象
  - 管道
      - 负责处理爬虫从Spider传递过来的Item对象，并将其持久化存储到文件或数据库中

- 请求传参
  - 使用场景：如果爬取解析数据不在一页中，而是需要通过参数来控制爬取的页码，那么可以通过请求传参的方式来实现
  - 实现思路：
    - 在start_requests方法中，生成初始请求时，将参数通过url传递给网站
    - 在parse方法中，解析页面数据时，从url中提取参数，并进行相应的处理
    - 案例需球：爬取boss的岗位名称，岗位描述